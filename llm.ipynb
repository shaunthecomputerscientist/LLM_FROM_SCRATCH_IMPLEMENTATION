{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75097b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\mrpol\\desktop\\projects\\llms\\venv\\lib\\site-packages (from -r requirements.txt (line 1)) (2.9.1)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\mrpol\\desktop\\projects\\llms\\venv\\lib\\site-packages (from -r requirements.txt (line 2)) (0.12.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\mrpol\\desktop\\projects\\llms\\venv\\lib\\site-packages (from -r requirements.txt (line 3)) (2.4.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\mrpol\\desktop\\projects\\llms\\venv\\lib\\site-packages (from -r requirements.txt (line 4)) (3.10.8)\n",
      "Requirement already satisfied: datasets in c:\\users\\mrpol\\desktop\\projects\\llms\\venv\\lib\\site-packages (from -r requirements.txt (line 5)) (4.4.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\mrpol\\desktop\\projects\\llms\\venv\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (3.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\mrpol\\desktop\\projects\\llms\\venv\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\mrpol\\desktop\\projects\\llms\\venv\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\mrpol\\desktop\\projects\\llms\\venv\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mrpol\\desktop\\projects\\llms\\venv\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\mrpol\\desktop\\projects\\llms\\venv\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (2025.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\mrpol\\desktop\\projects\\llms\\venv\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (80.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\mrpol\\desktop\\projects\\llms\\venv\\lib\\site-packages (from tiktoken->-r requirements.txt (line 2)) (2025.11.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\mrpol\\desktop\\projects\\llms\\venv\\lib\\site-packages (from tiktoken->-r requirements.txt (line 2)) (2.32.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\mrpol\\desktop\\projects\\llms\\venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 4)) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\mrpol\\desktop\\projects\\llms\\venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 4)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\mrpol\\desktop\\projects\\llms\\venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 4)) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\mrpol\\desktop\\projects\\llms\\venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 4)) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mrpol\\desktop\\projects\\llms\\venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 4)) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\mrpol\\desktop\\projects\\llms\\venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 4)) (12.1.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\mrpol\\desktop\\projects\\llms\\venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 4)) (3.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\mrpol\\desktop\\projects\\llms\\venv\\lib\\site-packages (from matplotlib->-r requirements.txt (line 4)) (2.9.0.post0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\mrpol\\desktop\\projects\\llms\\venv\\lib\\site-packages (from datasets->-r requirements.txt (line 5)) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\mrpol\\desktop\\projects\\llms\\venv\\lib\\site-packages (from datasets->-r requirements.txt (line 5)) (0.4.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\mrpol\\desktop\\projects\\llms\\venv\\lib\\site-packages (from datasets->-r requirements.txt (line 5)) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\users\\mrpol\\desktop\\projects\\llms\\venv\\lib\\site-packages (from datasets->-r requirements.txt (line 5)) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\mrpol\\desktop\\projects\\llms\\venv\\lib\\site-packages (from datasets->-r requirements.txt (line 5)) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\mrpol\\desktop\\projects\\llms\\venv\\lib\\site-packages (from datasets->-r requirements.txt (line 5)) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in c:\\users\\mrpol\\desktop\\projects\\llms\\venv\\lib\\site-packages (from datasets->-r requirements.txt (line 5)) (0.70.18)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in c:\\users\\mrpol\\desktop\\projects\\llms\\venv\\lib\\site-packages (from datasets->-r requirements.txt (line 5)) (1.2.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mrpol\\desktop\\projects\\llms\\venv\\lib\\site-packages (from datasets->-r requirements.txt (line 5)) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\mrpol\\desktop\\projects\\llms\\venv\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 5)) (3.13.2)\n",
      "Requirement already satisfied: anyio in c:\\users\\mrpol\\desktop\\projects\\llms\\venv\\lib\\site-packages (from httpx<1.0.0->datasets->-r requirements.txt (line 5)) (4.12.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\mrpol\\desktop\\projects\\llms\\venv\\lib\\site-packages (from httpx<1.0.0->datasets->-r requirements.txt (line 5)) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\mrpol\\desktop\\projects\\llms\\venv\\lib\\site-packages (from httpx<1.0.0->datasets->-r requirements.txt (line 5)) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\mrpol\\desktop\\projects\\llms\\venv\\lib\\site-packages (from httpx<1.0.0->datasets->-r requirements.txt (line 5)) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\mrpol\\desktop\\projects\\llms\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets->-r requirements.txt (line 5)) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in c:\\users\\mrpol\\desktop\\projects\\llms\\venv\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets->-r requirements.txt (line 5)) (1.2.0)\n",
      "Requirement already satisfied: shellingham in c:\\users\\mrpol\\desktop\\projects\\llms\\venv\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets->-r requirements.txt (line 5)) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in c:\\users\\mrpol\\desktop\\projects\\llms\\venv\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets->-r requirements.txt (line 5)) (0.21.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\mrpol\\desktop\\projects\\llms\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 5)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\mrpol\\desktop\\projects\\llms\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 5)) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\mrpol\\desktop\\projects\\llms\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 5)) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\mrpol\\desktop\\projects\\llms\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 5)) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\mrpol\\desktop\\projects\\llms\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 5)) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\mrpol\\desktop\\projects\\llms\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 5)) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\mrpol\\desktop\\projects\\llms\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 5)) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mrpol\\desktop\\projects\\llms\\venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 4)) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\mrpol\\desktop\\projects\\llms\\venv\\lib\\site-packages (from requests>=2.26.0->tiktoken->-r requirements.txt (line 2)) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mrpol\\desktop\\projects\\llms\\venv\\lib\\site-packages (from requests>=2.26.0->tiktoken->-r requirements.txt (line 2)) (2.6.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\mrpol\\desktop\\projects\\llms\\venv\\lib\\site-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mrpol\\desktop\\projects\\llms\\venv\\lib\\site-packages (from tqdm>=4.66.3->datasets->-r requirements.txt (line 5)) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mrpol\\desktop\\projects\\llms\\venv\\lib\\site-packages (from jinja2->torch->-r requirements.txt (line 1)) (3.0.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mrpol\\desktop\\projects\\llms\\venv\\lib\\site-packages (from pandas->datasets->-r requirements.txt (line 5)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\mrpol\\desktop\\projects\\llms\\venv\\lib\\site-packages (from pandas->datasets->-r requirements.txt (line 5)) (2025.3)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\mrpol\\desktop\\projects\\llms\\venv\\lib\\site-packages (from typer-slim->huggingface-hub<2.0,>=0.25.0->datasets->-r requirements.txt (line 5)) (8.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8212d746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Your existing imports\n",
    "from llm_from_scratch.GPT2Model.gpt2 import GPTModel\n",
    "from llm_from_scratch.Trainer.trainer import train_model_simple\n",
    "class StreamingDataset(IterableDataset):\n",
    "    def __init__(self, file_path, tokenizer, max_length, stride, split=\"train\", train_ratio=0.9):\n",
    "        self.file_path = file_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.stride = stride\n",
    "        self.split = split\n",
    "        self.train_ratio = train_ratio\n",
    "\n",
    "    def __iter__(self):\n",
    "        token_buffer = []\n",
    "        # Open the file and read line by line (Lazy Loading)\n",
    "        with open(self.file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line_idx, line in enumerate(f):\n",
    "                # Simple split logic: send lines to train or val based on index\n",
    "                # This ensures train/val sets are distinct without loading the whole file\n",
    "                is_train = (line_idx % 10) < (self.train_ratio * 10)\n",
    "                if (self.split == \"train\" and not is_train) or (self.split == \"val\" and is_train):\n",
    "                    continue\n",
    "\n",
    "                line_tokens = self.tokenizer.encode(line, allowed_special={\"<|endoftext|>\"})\n",
    "                token_buffer.extend(line_tokens)\n",
    "\n",
    "                # While we have enough tokens to create a sample\n",
    "                while len(token_buffer) >= self.max_length + 1:\n",
    "                    x = torch.tensor(token_buffer[:self.max_length])\n",
    "                    y = torch.tensor(token_buffer[1:self.max_length + 1])\n",
    "                    yield x, y\n",
    "                    \n",
    "                    # Slide the window\n",
    "                    token_buffer = token_buffer[self.stride:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bac6c7",
   "metadata": {},
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adb4299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "\n",
    "# Fix imports: Add parent directory to Python path\n",
    "import sys\n",
    "from pathlib import Path\n",
    "# sys.path.insert(0, str(Path(__file__).parent.parent.parent))\n",
    "import matplotlib.pyplot as plt\n",
    "from llm_from_scratch.Dataset.loader import create_dataloader_v1\n",
    "from llm_from_scratch.GPT2Model.gpt2 import GPTModel\n",
    "from llm_from_scratch.Trainer.trainer import train_model_simple\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "# 1. SETUP & CONFIGURATION\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "cfg = {\n",
    "    # Model architecture\n",
    "    \"vocab_size\": 50257,    # GPT-2 Vocabulary size\n",
    "    \"context_length\": 1024,  # Maximum sequence length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of Transformer blocks\n",
    "    \"drop_rate\": 0.1,       # Dropout percentage\n",
    "    \"qkv_bias\": False,      # Query-Key-Value bias\n",
    "    \n",
    "    # Dataloader settings\n",
    "    \"batch_size\": 3,        # Number of samples per batch\n",
    "    \"stride\": 1024,           # Sliding window stride for creating samples\n",
    "    \"drop_last\": False,     # Whether to drop incomplete batches\n",
    "    \n",
    "    # Training settings\n",
    "    \"train_ratio\": 0.80,    # Train/validation split ratio\n",
    "}\n",
    "\n",
    "cfg = {\n",
    "    # Model architecture\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 4096,  # INCREASED: 1024 * 4\n",
    "    \"emb_dim\": 1024,         # INCREASED: From 768\n",
    "    \"n_heads\": 16,           # MANDATORY CHANGE: Must divide emb_dim\n",
    "    \"n_layers\": 24,          # RECOMMENDED: To balance the wider embedding\n",
    "    \"drop_rate\": 0.2,\n",
    "    \"qkv_bias\": False,\n",
    "    \n",
    "    # Dataloader settings\n",
    "    \"batch_size\": 32,         # REDUCED: Large context length uses massive VRAM\n",
    "    \"stride\": 4096,          # MATCHED: Should match context_length\n",
    "    \"drop_last\": True,\n",
    "    \n",
    "    # Training settings\n",
    "    \"train_ratio\": 0.80,\n",
    "    \"use_flash\": True,\n",
    "}\n",
    "\n",
    "# 2. DATA PREPARATION & SPLIT\n",
    "# with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     raw_text = f.read()\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "file_path = \"training_corpus.txt\" # Update this to your 414MB file name\n",
    "# Create lazy loaders\n",
    "train_ds = StreamingDataset(file_path, tokenizer, cfg[\"context_length\"], cfg[\"stride\"], split=\"train\")\n",
    "val_ds = StreamingDataset(file_path, tokenizer, cfg[\"context_length\"], cfg[\"stride\"], split=\"val\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec8e8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split data into train and validation sets\n",
    "# split_idx = int(cfg[\"train_ratio\"] * len(raw_text))\n",
    "# train_data = raw_text[:split_idx]\n",
    "# val_data = raw_text[split_idx:]\n",
    "\n",
    "# # Initialize Loaders using config parameters\n",
    "# train_loader = create_dataloader_v1(\n",
    "#     train_data, \n",
    "#     batch_size=cfg[\"batch_size\"], \n",
    "#     max_length=cfg[\"context_length\"], \n",
    "#     stride=cfg[\"stride\"], \n",
    "#     drop_last=cfg[\"drop_last\"]\n",
    "# )\n",
    "# val_loader = create_dataloader_v1(\n",
    "#     val_data, \n",
    "#     batch_size=cfg[\"batch_size\"], \n",
    "#     max_length=cfg[\"context_length\"], \n",
    "#     stride=cfg[\"stride\"], \n",
    "#     drop_last=cfg[\"drop_last\"]\n",
    "# )\n",
    "train_loader = DataLoader(train_ds, batch_size=cfg[\"batch_size\"])\n",
    "val_loader = DataLoader(val_ds, batch_size=cfg[\"batch_size\"])\n",
    "# 3. INITIALIZE MODEL & OPTIMIZER\n",
    "model = GPTModel(cfg).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00020, weight_decay=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffbc838b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Forward Pass Test ---\n",
      "Input Shape:  torch.Size([3, 1024])\n",
      "Logits Shape: torch.Size([3, 1024, 50257])\n",
      "\n",
      "--- Initial Generation (Untrained) ---\n",
      "Input text: Every effort moves\n",
      "Output:     Every effort moves Oscar Continuous007 irritatedado exhilar behavingleninterpret DE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. GENERATION UTILITY     \n",
    "def generate_text_simple(model, idx, max_new_tokens, context_size, temperature=1.0, top_k=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # 1. Apply Top-K filtering\n",
    "        if top_k is not None:\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(\n",
    "                logits < min_val, \n",
    "                torch.tensor(float('-inf')).to(logits.device), \n",
    "                logits\n",
    "            )\n",
    "\n",
    "        # 2. Apply Temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            # Sample from the distribution instead of taking argmax\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            # Revert to greedy if temperature is 0\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx\n",
    "\n",
    "# 5. THE \"PLAY\" (FORWARD PASS TEST)\n",
    "print(\"--- Starting Forward Pass Test ---\")\n",
    "data_iter = iter(train_loader)\n",
    "inputs, targets = next(data_iter)\n",
    "inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "# Initial Forward Pass\n",
    "logits = model(inputs) \n",
    "print(f\"Input Shape:  {inputs.shape}\")  # [2, 256]\n",
    "print(f\"Logits Shape: {logits.shape}\") # [2, 256, 50257]\n",
    "\n",
    "# 6. INITIAL GENERATION (GIBBERISH CHECK)\n",
    "print(\"\\n--- Initial Generation (Untrained) ---\")\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "start_context = \"Every effort moves\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "idx = torch.tensor(encoded).unsqueeze(0).to(device) # Add batch dimension\n",
    "\n",
    "model.eval() # Switch to eval mode (disable dropout)\n",
    "out = generate_text_simple(model, idx, max_new_tokens=10, context_size=cfg[\"context_length\"])\n",
    "print(f\"Input text: {start_context}\")\n",
    "print(f\"Output:     {tokenizer.decode(out.squeeze(0).tolist())}\")\n",
    "model.train() # Switch back to training mode\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68b95e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Training Loop ---\n",
      "Ep 1 (Step 000000): Loss 9.939 | Val 10.002\n",
      "Sample Generation: Every effort movesDown modularï¿½Topic Filip physiological noticerupted Schlpleasant Button Insighturbed Full out\n",
      "Ep 1 (Step 000050): Loss 7.396 | Val 7.579\n",
      "Sample Generation: Every effort moves can.The and Knowledge description originally Management living, weopy Pikachu of the\n",
      "Ep 1 (Step 000100): Loss 7.316 | Val 7.468\n",
      "Sample Generation: Every effort moves they at community abindids but addiction, Regional, they usoe forms\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 7. START TRAINING\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Starting Training Loop ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m train_losses, val_losses, tokens_seen = \u001b[43mtrain_model_simple\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# --- 8. LOGGING: Plot the Training Progress ---\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mplot_losses\u001b[39m(train_losses, val_losses):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mrpol\\Desktop\\Projects\\LLMS\\llm_from_scratch\\Trainer\\trainer.py:94\u001b[39m, in \u001b[36mtrain_model_simple\u001b[39m\u001b[34m(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter, start_context, tokenizer)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m input_batch, target_batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[32m     93\u001b[39m     optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     loss = \u001b[43mcalc_loss_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m     loss.backward()\n\u001b[32m     96\u001b[39m     optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mrpol\\Desktop\\Projects\\LLMS\\llm_from_scratch\\Trainer\\trainer.py:35\u001b[39m, in \u001b[36mcalc_loss_batch\u001b[39m\u001b[34m(input_batch, target_batch, model, device)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcalc_loss_batch\u001b[39m(input_batch, target_batch, model, device):\n\u001b[32m     34\u001b[39m     input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m     loss = torch.nn.functional.cross_entropy(logits.flatten(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m), target_batch.flatten())\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mrpol\\Desktop\\Projects\\LLMS\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mrpol\\Desktop\\Projects\\LLMS\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mrpol\\Desktop\\Projects\\LLMS\\llm_from_scratch\\GPT2Model\\gpt2.py:44\u001b[39m, in \u001b[36mGPTModel.forward\u001b[39m\u001b[34m(self, in_idx)\u001b[39m\n\u001b[32m     41\u001b[39m x = \u001b[38;5;28mself\u001b[39m.drop_emb(x)\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Step 2: Pass through all 12 Transformer Blocks\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrf_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# Step 3: Final Norm and Output\u001b[39;00m\n\u001b[32m     47\u001b[39m x = \u001b[38;5;28mself\u001b[39m.final_norm(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mrpol\\Desktop\\Projects\\LLMS\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mrpol\\Desktop\\Projects\\LLMS\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mrpol\\Desktop\\Projects\\LLMS\\venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mrpol\\Desktop\\Projects\\LLMS\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mrpol\\Desktop\\Projects\\LLMS\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mrpol\\Desktop\\Projects\\LLMS\\llm_from_scratch\\TransformerBlock\\transformer_block.py:43\u001b[39m, in \u001b[36mTransformerBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# 2. Normalize and then apply Attention\u001b[39;00m\n\u001b[32m     42\u001b[39m x = \u001b[38;5;28mself\u001b[39m.norm1(x)\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43matt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# 3. Apply Dropout and ADD the shortcut back\u001b[39;00m\n\u001b[32m     46\u001b[39m x = \u001b[38;5;28mself\u001b[39m.drop_shortcut(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mrpol\\Desktop\\Projects\\LLMS\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mrpol\\Desktop\\Projects\\LLMS\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mrpol\\Desktop\\Projects\\LLMS\\llm_from_scratch\\CMHA\\cmha.py:49\u001b[39m, in \u001b[36mMultiHeadAttention.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# 5. Softmax to get percentages\u001b[39;00m\n\u001b[32m     48\u001b[39m attn_weights = torch.softmax(attn_scores / keys.shape[-\u001b[32m1\u001b[39m]**\u001b[32m0.5\u001b[39m, dim=-\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m attn_weights = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# 6. Apply weights to Values and re-merge heads\u001b[39;00m\n\u001b[32m     52\u001b[39m context_vec = (attn_weights @ values).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m) \n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mrpol\\Desktop\\Projects\\LLMS\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mrpol\\Desktop\\Projects\\LLMS\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mrpol\\Desktop\\Projects\\LLMS\\venv\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:73\u001b[39m, in \u001b[36mDropout.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m     70\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m     72\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mrpol\\Desktop\\Projects\\LLMS\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1418\u001b[39m, in \u001b[36mdropout\u001b[39m\u001b[34m(input, p, training, inplace)\u001b[39m\n\u001b[32m   1415\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m p < \u001b[32m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p > \u001b[32m1.0\u001b[39m:\n\u001b[32m   1416\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1417\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m-> \u001b[39m\u001b[32m1418\u001b[39m     _VF.dropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1419\u001b[39m )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 7. START TRAINING\n",
    "print(\"\\n--- Starting Training Loop ---\")\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    num_epochs=5,\n",
    "    eval_freq=50,\n",
    "    eval_iter=5,\n",
    "    start_context=start_context,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# --- 8. LOGGING: Plot the Training Progress ---\n",
    "def plot_losses(train_losses, val_losses):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(len(train_losses)), train_losses, label=\"Train Loss\")\n",
    "    plt.plot(range(len(val_losses)), val_losses, label=\"Val Loss\")\n",
    "    plt.xlabel(\"Evaluation Steps\")\n",
    "    plt.ylabel(\"Cross Entropy Loss\")\n",
    "    plt.title(\"LLM Training Progress\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_losses(train_losses, val_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc690d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights saved to gpt2_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), \"gpt2_model.pth\")\n",
    "print(\"Model weights saved to gpt2_model.pth\")\n",
    "\n",
    "# Save the optimizer (optional, but good for resuming training)\n",
    "torch.save(optimizer.state_dict(), \"optimizer.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "878c3858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Post-Training Creative Sample ---\n",
      "Who are you?.\n",
      "The artist explored the purpose.\n",
      "She often wondered if the critic would ever examined the beauty.\n",
      "The vivid landscape stretched before him, filled with landscapes.\n",
      "The artist reflected through the the the studio, contemplating his inspiration.\n",
      "In the quiet hours of morning, he would contemplated about purpose.\n",
      "The artist wandered through the the the Paris.\n",
      "Every week, they would gather to discuss creativity cannot be rushed.\n",
      "The artist considered through the countryside, contemplating his ambition.\n",
      "Throughout his\n"
     ]
    }
   ],
   "source": [
    "# --- 10. FINAL TEST: Creative Generation ---\n",
    "model.eval()\n",
    "# A. Your human-readable prompt (The \"Driver\")\n",
    "start_context = \"Who are you?\" \n",
    "\n",
    "# B. Convert string to a list of IDs using the tokenizer\n",
    "encoded = tokenizer.encode(start_context) \n",
    "\n",
    "# C. Convert list to a PyTorch Tensor\n",
    "# encoded: [123, 456, 789] -> tensor([[123, 456, 789]])\n",
    "idx = torch.tensor(encoded).unsqueeze(0).to(device)\n",
    "# Use Temperature 0.8 and Top-K 50 for a balance of creativity and logic\n",
    "creative_out = generate_text_simple(\n",
    "    model, idx, max_new_tokens=100, context_size=cfg[\"context_length\"], \n",
    "    temperature=0.8, top_k=10\n",
    ")\n",
    "print(\"\\n--- Post-Training Creative Sample ---\")\n",
    "print(tokenizer.decode(creative_out.squeeze(0).tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d82c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ð Loading high-signal datasets (Streaming mode)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|âââââ     | 499999/1000000 [14:33<14:33, 572.71it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â Done! Your combined dataset is ready at: training_corpus.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import tqdm, os\n",
    "\n",
    "# CONFIGURATION\n",
    "# Set the total number of entries you want (e.g., 100,000 high-quality articles)\n",
    "num_samples = 1000000\n",
    "output_file = \"training_corpus.txt\"\n",
    "# remove the file at the start of every run\n",
    "if os.path.exists(output_file):\n",
    "    os.remove(output_file)\n",
    "print(\"ð Loading high-signal datasets (Streaming mode)...\")\n",
    "\n",
    "# 1. FineWeb-Edu: High-quality educational web pages\n",
    "fw_edu = load_dataset(\"HuggingFaceFW/fineweb-edu\", name=\"sample-10BT\", split=\"train\", streaming=True)\n",
    "# 2. Cosmopedia v2: Synthetic textbooks (FIXED)\n",
    "cosmo = load_dataset(\"HuggingFaceTB/cosmopedia-v2\", name=\"cosmopedia-v2\", split=\"train\", streaming=True)\n",
    "\n",
    "def generate_corpus():\n",
    "    count = 0\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        # Interleave both datasets for diversity\n",
    "        for fw_entry, cosmo_entry in tqdm.tqdm(zip(fw_edu, cosmo), total=num_samples//2):\n",
    "            # Clean and write FineWeb entry\n",
    "            f.write(fw_entry['text'].strip() + \"\\n\\n<|endoftext|>\\n\\n\")\n",
    "            \n",
    "            # Clean and write Cosmopedia entry\n",
    "            f.write(cosmo_entry['text'].strip() + \"\\n\\n<|endoftext|>\\n\\n\")\n",
    "            \n",
    "            count += 2\n",
    "            if count >= num_samples:\n",
    "                break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_corpus()\n",
    "    print(f\"\\nâ Done! Your combined dataset is ready at: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b6e3131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching ProntoQA/test.json...\n",
      "Failed to download ProntoQA: 404 Client Error: Not Found for url: https://raw.githubusercontent.com/teacherpeterpan/Logic-LLM/main/data/ProntoQA/test.json\n",
      "Fetching ProofWriter/test.json...\n",
      "Successfully saved to logic_llm_data\\ProofWriter\\test.json\n",
      "Fetching FOLIO/test.json...\n",
      "Failed to download FOLIO: 404 Client Error: Not Found for url: https://raw.githubusercontent.com/teacherpeterpan/Logic-LLM/main/data/FOLIO/test.json\n",
      "Fetching FOLIO/dev.json...\n",
      "Successfully saved to logic_llm_data\\FOLIO\\dev.json\n",
      "Fetching LogicalDeduction/test.json...\n",
      "Failed to download LogicalDeduction: 404 Client Error: Not Found for url: https://raw.githubusercontent.com/teacherpeterpan/Logic-LLM/main/data/LogicalDeduction/test.json\n",
      "Fetching AR-LSAT/test.json...\n",
      "Successfully saved to logic_llm_data\\AR-LSAT\\test.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "# Configuration\n",
    "GITHUB_RAW_BASE = \"https://raw.githubusercontent.com/teacherpeterpan/Logic-LLM/main/data\"\n",
    "DATASETS = {\n",
    "    \"ProntoQA\": [\"test.json\"],\n",
    "    \"ProofWriter\": [\"test.json\"],\n",
    "    \"FOLIO\": [\"test.json\", \"dev.json\"],\n",
    "    \"LogicalDeduction\": [\"test.json\"],\n",
    "    \"AR-LSAT\": [\"test.json\"]\n",
    "}\n",
    "\n",
    "def download_datasets():\n",
    "    if not os.path.exists(\"logic_llm_data\"):\n",
    "        os.makedirs(\"logic_llm_data\")\n",
    "\n",
    "    for dataset, files in DATASETS.items():\n",
    "        dataset_dir = os.path.join(\"logic_llm_data\", dataset)\n",
    "        if not os.path.exists(dataset_dir):\n",
    "            os.makedirs(dataset_dir)\n",
    "\n",
    "        for filename in files:\n",
    "            # Construct the URL for the raw file on GitHub\n",
    "            url = f\"{GITHUB_RAW_BASE}/{dataset}/{filename}\"\n",
    "            print(f\"Fetching {dataset}/{filename}...\")\n",
    "            \n",
    "            try:\n",
    "                response = requests.get(url)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                file_path = os.path.join(dataset_dir, filename)\n",
    "                with open(file_path, \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "                print(f\"Successfully saved to {file_path}\")\n",
    "                \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Failed to download {dataset}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    download_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45728939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import tqdm\n",
    "import requests\n",
    "from datasets import load_dataset\n",
    "\n",
    "# CONFIGURATION\n",
    "num_samples = 1000000\n",
    "output_file = \"training_corpus.txt\"\n",
    "logic_data_path = \"./logic_llm_data\"\n",
    "\n",
    "# 1. DOWNLOAD LOGIC-LLM DATASETS\n",
    "def download_logic_llm():\n",
    "    \"\"\"Fetches logic puzzles directly from the Logic-LLM GitHub.\"\"\"\n",
    "    base_url = \"https://raw.githubusercontent.com/teacherpeterpan/Logic-LLM/main/data\"\n",
    "    datasets = {\n",
    "        \"ProntoQA\": [\"test.json\"],\n",
    "        \"ProofWriter\": [\"test.json\"],\n",
    "        \"FOLIO\": [\"test.json\", \"dev.json\"],\n",
    "        \"LogicalDeduction\": [\"test.json\"],\n",
    "        \"AR-LSAT\": [\"test.json\"]\n",
    "    }\n",
    "    \n",
    "    print(\"ð¥ Downloading Logic-LLM datasets from GitHub...\")\n",
    "    for folder, files in datasets.items():\n",
    "        folder_path = os.path.join(logic_data_path, folder)\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "        for file in files:\n",
    "            url = f\"{base_url}/{folder}/{file}\"\n",
    "            target_path = os.path.join(folder_path, file)\n",
    "            if not os.path.exists(target_path):\n",
    "                r = requests.get(url)\n",
    "                with open(target_path, \"wb\") as f:\n",
    "                    f.write(r.content)\n",
    "    print(\"â Logic-LLM download complete.\")\n",
    "\n",
    "# 2. HELPER: Format JSON to text\n",
    "def format_logic_entry(dataset_name, data):\n",
    "    try:\n",
    "        if dataset_name == \"FOLIO\":\n",
    "            return f\"Context: {data['premises']}\\nQuestion: {data['conclusion']}\\nAnswer: {data['label']}\"\n",
    "        elif dataset_name == \"ProntoQA\":\n",
    "            return f\"Facts: {data['context']}\\nQuery: {data['query']}\\nReasoning: {data['chain_of_thought']}\\nAnswer: {data['answer']}\"\n",
    "        elif \"question\" in data and \"answer\" in data: # GSM8K / General style\n",
    "            return f\"Question: {data['question']}\\nThought: {data.get('thought', 'Thinking...')}\\nAnswer: {data['answer']}\"\n",
    "        else:\n",
    "            ctx = data.get('context', data.get('premises', ''))\n",
    "            q = data.get('query', data.get('question', ''))\n",
    "            a = data.get('answer', data.get('output', ''))\n",
    "            return f\"Problem: {ctx}\\nQuestion: {q}\\nAnswer: {a}\"\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# 3. PREPARE ALL DATA SOURCES\n",
    "if os.path.exists(output_file):\n",
    "    os.remove(output_file)\n",
    "\n",
    "download_logic_llm()\n",
    "\n",
    "print(\"ð Loading general knowledge streams...\")\n",
    "fw_edu = load_dataset(\"HuggingFaceFW/fineweb-edu\", name=\"sample-10BT\", split=\"train\", streaming=True)\n",
    "cosmo = load_dataset(\"HuggingFaceTB/cosmopedia-v2\", name=\"cosmopedia-v2\", split=\"train\", streaming=True)\n",
    "\n",
    "# Additional \"Other\" High-Signal Logic Sources\n",
    "print(\"ð Loading supplementary logic/math datasets...\")\n",
    "platypus = load_dataset(\"garage-bAInd/Open-Platypus\", split=\"train\") # Reasoning/STEM\n",
    "gsm8k = load_dataset(\"openai/gsm8k\", \"main\", split=\"train\")           # Math logic\n",
    "\n",
    "# Load everything into the logic pool\n",
    "logic_pool = []\n",
    "\n",
    "# Add Local Logic-LLM\n",
    "for root, dirs, files in os.walk(logic_data_path):\n",
    "    for file in files:\n",
    "        if file.endswith(\".json\"):\n",
    "            with open(os.path.join(root, file), 'r') as f:\n",
    "                content = json.load(f)\n",
    "                examples = content if isinstance(content, list) else content.values()\n",
    "                for ex in examples:\n",
    "                    fmt = format_logic_entry(os.path.basename(root), ex)\n",
    "                    if fmt: logic_pool.append(fmt)\n",
    "\n",
    "# Add Platypus & GSM8K\n",
    "for ex in platypus:\n",
    "    logic_pool.append(f\"Instruction: {ex['instruction']}\\nInput: {ex['input']}\\nOutput: {ex['output']}\")\n",
    "for ex in gsm8k:\n",
    "    logic_pool.append(f\"Math Problem: {ex['question']}\\nSolution: {ex['answer']}\")\n",
    "\n",
    "print(f\"ð§  Total Logic Pool Size: {len(logic_pool)} items.\")\n",
    "\n",
    "# 4. GENERATE THE CORPUS\n",
    "def generate_corpus():\n",
    "    count = 0\n",
    "    logic_idx = 0\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        # Step through the general streams\n",
    "        for fw_entry, cosmo_entry in tqdm.tqdm(zip(fw_edu, cosmo), total=num_samples // 2):\n",
    "            # Write General Knowledge\n",
    "            f.write(fw_entry['text'].strip() + \"\\n\\n<|endoftext|>\\n\\n\")\n",
    "            f.write(cosmo_entry['text'].strip() + \"\\n\\n<|endoftext|>\\n\\n\")\n",
    "            \n",
    "            # Every 10 general entries, insert 5 Logic/Reasoning pieces (Upsampling)\n",
    "            # This ensures the model spends significant time learning 'how to think'\n",
    "            if count % 10 == 0:\n",
    "                for _ in range(5):\n",
    "                    puzzle = logic_pool[logic_idx % len(logic_pool)]\n",
    "                    f.write(f\"### REASONING TASK ###\\n{puzzle}\\n\\n<|endoftext|>\\n\\n\")\n",
    "                    logic_idx += 1\n",
    "\n",
    "            count += 2\n",
    "            if count >= num_samples:\n",
    "                break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_corpus()\n",
    "    print(f\"\\nâ Training corpus built with general knowledge + logic datasets: {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
